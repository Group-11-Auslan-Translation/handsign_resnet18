{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aacca3d-3da2-44d4-bf28-c6dc1b3a8218",
   "metadata": {},
   "source": [
    "# Training a ResNet-18 model on grayscale images using Auslan Dataset from Kaggle.\n",
    "\n",
    "# 1. Importing Required Libraries\n",
    "\n",
    "In this section, we import the necessary libraries for:\n",
    "- **Loading datasets**: `torchvision.datasets`, `DataLoader`\n",
    "- **Data augmentation and preprocessing**: `torchvision.transforms`\n",
    "- **Building and training the model**: `torch`, `torchvision.models`, `optim`, `nn`\n",
    "- **Progress tracking**: `tqdm` for displaying progress bars during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f4873c-9db4-4278-9b0c-764ec3892ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2fff9-180a-425b-b9a9-18b00c687c2c",
   "metadata": {},
   "source": [
    "# Weights and Biases initialisation\n",
    "\n",
    "Initialize W&B. Log hyperparameters to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ffe0abb-63e2-4b5c-aec6-01bb7a70324d",
   "metadata": {},
  
   "source": [
    "\n",
    "# Set your W&B API key (replace with your actual API key from your W&B account)\n",
    "os.environ[\"WANDB_API_KEY\"] = \"40605c764bdfbd3ad65c3f127ed7d9dbdd4ce87f\"\n",
    "\n",
    "# Login again with the new API key\n",
    "wandb.login(relogin=True)\n",
    "wandb.init(project=\"auslan-handsign-classification\",\n",
    "\n",
    "# Log hyperparameters to W&B\n",
    "     config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 25,\n",
    "    \"batch_size\": 64,\n",
    "    \"architecture\": \"ResNet18\",\n",
    "    \"dataset\": \"Auslan Hand Signs\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb6420-f6d1-4691-bd2e-52640338fc9c",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation and Preprocessing\n",
    "\n",
    "In this section, we define the transformations that will be applied to the images before they are fed into the model:\n",
    "- **Grayscale conversion**: Convert the images to grayscale.\n",
    "- **Random augmentations**: Random resized cropping, horizontal flipping, and rotation are applied to the training images to help the model generalize better.\n",
    "- **Random Erasing**: This helps the model learn to handle occlusions and missing parts in the input images.\n",
    "- **Normalization**: Normalizing the pixel values between 0 and 1 is important for stabilizing the model's learning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "447ca4e7-1289-4bb8-b936-65c074622a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing for the training and validation sets\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.RandomErasing(p=0.2),  # Randomly erase part of the tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize for grayscale (1 channel)\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize for grayscale (1 channel)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716f193-5889-400f-bafb-91c005f158ae",
   "metadata": {},
   "source": [
    "# 3. Loading Datasets and Creating DataLoaders\n",
    "\n",
    "- **ImageFolder**: Automatically assigns labels based on the subfolder names, which represent the class names.\n",
    "- **DataLoader**: Loads batches of data from the `train` and `val` folders. It shuffles the training data to introduce randomness in batches, while validation data is loaded in a fixed order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825b3d5c-5726-44b0-b2a1-61e561dc7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"C:\\Users\\zed20\\Documents\\Auslan_dataset\\dataset_split\\train\"\n",
    "val_dir = r\"C:\\Users\\zed20\\Documents\\Auslan_dataset\\dataset_split\\val\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7dd5d-5f55-4374-96da-d76281057477",
   "metadata": {},
   "source": [
    "# 4. Model Setup (ResNet-18 with Grayscale Input Modification)\n",
    "\n",
    "- We load a pre-trained ResNet-18 model that has been trained on ImageNet.\n",
    "- Modify the **first convolutional layer** to accept 1-channel (grayscale) images instead of 3-channel (RGB) images.\n",
    "- Modify the **fully connected (FC) layer** to output the correct number of classes (36 in this case).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a41178-e5ec-4d91-9244-d6a539e4f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load ResNet-18 model with ImageNet pre-trained weights\n",
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify the first layer to accept grayscale input\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Modify the FC layer for 36 output classes (26 letters + 10 digits)\n",
    "num_classes = 36\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, num_classes)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc863dd-c7a0-4c10-afb5-eec9a3218940",
   "metadata": {},
   "source": [
    "# 5. Defining Loss Function and Optimizer\n",
    "\n",
    "- **CrossEntropyLoss**: This is used for multi-class classification tasks, where the model predicts one out of multiple classes.\n",
    "- **AdamW Optimizer**: Used for weight decay and stability in optimization.\n",
    "- **OneCycleLR Scheduler**: This dynamically adjusts the learning rate throughout the training process to help the model converge faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3610ba76-d5ea-424c-89a1-2588c57d3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                epochs=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bcdbd1-c19c-4f58-b484-361682432422",
   "metadata": {},
   "source": [
    "# 6. Validation Function\n",
    "\n",
    "This function performs validation on the model after each training epoch. It:\n",
    "- Disables gradient calculation using `torch.no_grad()`.\n",
    "- Calculates the validation loss and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbc4cf58-7ed6-40e2-aa58-c2a98d76ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store for classification report\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    return val_loss, val_acc, y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943365c-c653-4891-aa59-1052cf58b92a",
   "metadata": {},
   "source": [
    "# 7. Training Loop with Early Stopping and Logging confusion matrix\n",
    "\n",
    "The training loop:\n",
    "- Trains the model over a number of epochs.\n",
    "- After each epoch, it performs validation.\n",
    "- Uses early stopping if the validation accuracy does not improve for a set number of epochs (`patience`).\n",
    "- Saves the model with the best validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd478797-8f42-468b-b669-7028c7324e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=5):\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        val_loss, val_acc, y_true, y_pred = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        wandb.log({\n",
    "            \"train_loss\": running_loss / len(train_loader.dataset),\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        # Log classification metrics (F1-score, precision, recall)\n",
    "        classification_report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "        wandb.log({\n",
    "            \"precision\": classification_report_dict[\"macro avg\"][\"precision\"],\n",
    "            \"recall\": classification_report_dict[\"macro avg\"][\"recall\"],\n",
    "            \"f1-score\": classification_report_dict[\"macro avg\"][\"f1-score\"],\n",
    "            \"accuracy\": classification_report_dict[\"accuracy\"]\n",
    "        })\n",
    "\n",
    "        # Log confusion matrix (optional)\n",
    "        wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                  y_true=y_true,\n",
    "                                                                  preds=y_pred,\n",
    "                                                                  class_names=[str(i) for i in range(36)])})\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_resnet18_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {running_loss / len(train_loader.dataset):.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete. Best Val Acc: {:.2f}%\".format(best_val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc567d65-011e-4e39-9e4f-d00ad8191d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c808f3b-b5bc-4ca6-bf66-f2648215f6d8",
   "metadata": {},
   "source": [
    "# 8. Training the Model\n",
    "\n",
    "This is the final step where we train the model using the `train_model` function defined earlier. The model is saved as `resnet18_handsign_final.pth` once training is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c480444-84b8-4f5b-8cfa-7f0b7a025cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Training: 100%|█████████████████████████████████████████████████████████| 780/780 [04:32<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.3058, Train Acc: 30.74%\n",
      "Val Loss: 0.9557, Val Acc: 68.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Training: 100%|█████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.2762, Train Acc: 59.92%\n",
      "Val Loss: 0.5954, Val Acc: 79.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Training: 100%|█████████████████████████████████████████████████████████| 780/780 [04:23<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.9749, Train Acc: 69.49%\n",
      "Val Loss: 0.5422, Val Acc: 81.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Training: 100%|█████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.8316, Train Acc: 74.11%\n",
      "Val Loss: 0.3846, Val Acc: 87.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Training: 100%|█████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.7390, Train Acc: 77.45%\n",
      "Val Loss: 0.2846, Val Acc: 90.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Training: 100%|█████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.6539, Train Acc: 79.85%\n",
      "Val Loss: 0.3135, Val Acc: 89.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Training: 100%|█████████████████████████████████████████████████████████| 780/780 [04:23<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.6141, Train Acc: 81.02%\n",
      "Val Loss: 0.2736, Val Acc: 90.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Training: 100%|█████████████████████████████████████████████████████████| 780/780 [04:23<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.5696, Train Acc: 82.59%\n",
      "Val Loss: 0.2297, Val Acc: 92.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Training: 100%|█████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.5412, Train Acc: 83.41%\n",
      "Val Loss: 0.2214, Val Acc: 92.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Training: 100%|████████████████████████████████████████████████████████| 780/780 [04:23<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.5147, Train Acc: 84.19%\n",
      "Val Loss: 0.1920, Val Acc: 93.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Training: 100%|████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 0.4914, Train Acc: 85.01%\n",
      "Val Loss: 0.1350, Val Acc: 95.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Training: 100%|████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 0.4724, Train Acc: 85.63%\n",
      "Val Loss: 0.1359, Val Acc: 95.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Training: 100%|████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 0.4572, Train Acc: 85.97%\n",
      "Val Loss: 0.1578, Val Acc: 94.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Training: 100%|████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss: 0.4342, Train Acc: 86.64%\n",
      "Val Loss: 0.1115, Val Acc: 96.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Training: 100%|████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 0.4233, Train Acc: 87.13%\n",
      "Val Loss: 0.1266, Val Acc: 95.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Training: 100%|████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train Loss: 0.4137, Train Acc: 87.31%\n",
      "Val Loss: 0.1419, Val Acc: 95.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Training: 100%|████████████████████████████████████████████████████████| 780/780 [04:24<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train Loss: 0.3980, Train Acc: 87.90%\n",
      "Val Loss: 0.1155, Val Acc: 96.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Training: 100%|████████████████████████████████████████████████████████| 780/780 [04:21<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train Loss: 0.3829, Train Acc: 88.15%\n",
      "Val Loss: 0.1119, Val Acc: 95.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - Training: 100%|████████████████████████████████████████████████████████| 780/780 [04:22<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered\n",
      "Training complete. Best Val Acc: 96.26%\n",
      "Model saved to resnet18_handsign_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to resnet18_handsign_final.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Finalize the W&B run\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:4243\u001b[0m, in \u001b[0;36mfinish\u001b[1;34m(exit_code, quiet)\u001b[0m\n\u001b[0;32m   4233\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[0;32m   4234\u001b[0m \n\u001b[0;32m   4235\u001b[0m \u001b[38;5;124;03mThis is used when creating multiple runs in the same process.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4240\u001b[0m \u001b[38;5;124;03m    quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[0;32m   4241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mrun:\n\u001b[1;32m-> 4243\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:452\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:393\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2157\u001b[0m, in \u001b[0;36mRun.finish\u001b[1;34m(self, exit_code, quiet)\u001b[0m\n\u001b[0;32m   2143\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_noop\n\u001b[0;32m   2144\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_attach\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinish\u001b[39m(\n\u001b[0;32m   2146\u001b[0m     \u001b[38;5;28mself\u001b[39m, exit_code: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, quiet: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[0;32m   2149\u001b[0m \n\u001b[0;32m   2150\u001b[0m \u001b[38;5;124;03m    This is used when creating multiple runs in the same process. We automatically\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;124;03m        quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[0;32m   2156\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2191\u001b[0m, in \u001b[0;36mRun._finish\u001b[1;34m(self, exit_code, quiet)\u001b[0m\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2191\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_atexit_cleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;66;03m# Run hooks that should happen after the last messages to the\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;66;03m# internal service, like detaching the logger.\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown_hooks:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2440\u001b[0m, in \u001b[0;36mRun._atexit_cleanup\u001b[1;34m(self, exit_code)\u001b[0m\n\u001b[0;32m   2437\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39mresume_fname)\n\u001b[0;32m   2439\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2440\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   2443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mwandb_agent\u001b[38;5;241m.\u001b[39m_is_running():  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2682\u001b[0m, in \u001b[0;36mRun._on_finish\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2679\u001b[0m exit_handle\u001b[38;5;241m.\u001b[39madd_probe(on_probe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_probe_exit)\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;66;03m# wait for the exit to complete\u001b[39;00m\n\u001b[1;32m-> 2682\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mexit_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_exit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2684\u001b[0m poll_exit_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_poll_exit()\n\u001b[0;32m   2685\u001b[0m \u001b[38;5;66;03m# wait for them, it's ok to do this serially but this can be improved\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[1;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[0;32m    129\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=5)\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(model.state_dict(), 'resnet18_handsign_final.pth')\n",
    "print(\"Model saved to resnet18_handsign_final.pth\")\n",
    "\n",
    "# Finalize the W&B run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
