{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aacca3d-3da2-44d4-bf28-c6dc1b3a8218",
   "metadata": {},
   "source": [
    "# Training a ResNet-18 model on grayscale images using Auslan Dataset from Kaggle.\n",
    "\n",
    "# 1. Importing Required Libraries\n",
    "\n",
    "In this section, we import the necessary libraries for:\n",
    "- **Loading datasets**: `torchvision.datasets`, `DataLoader`\n",
    "- **Data augmentation and preprocessing**: `torchvision.transforms`\n",
    "- **Building and training the model**: `torch`, `torchvision.models`, `optim`, `nn`\n",
    "- **Progress tracking**: `tqdm` for displaying progress bars during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f4873c-9db4-4278-9b0c-764ec3892ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2fff9-180a-425b-b9a9-18b00c687c2c",
   "metadata": {},
   "source": [
    "# Weights and Biases initialisation\n",
    "\n",
    "Initialize W&B. Log hyperparameters to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe0abb-63e2-4b5c-aec6-01bb7a70324d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set your W&B API key (replace with your actual API key from your W&B account)\n",
    "os.environ[\"WANDB_API_KEY\"] = \"40605c764bdfbd....7d9dbdd4ce87f\"\n",
    "\n",
    "# Login again with the new API key\n",
    "wandb.login(relogin=True)\n",
    "wandb.init(project=\"auslan-handsign-classification\",\n",
    "\n",
    "# Log hyperparameters to W&B\n",
    "     config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 25,\n",
    "    \"batch_size\": 64,\n",
    "    \"architecture\": \"ResNet18\",\n",
    "    \"dataset\": \"Auslan Hand Signs\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb6420-f6d1-4691-bd2e-52640338fc9c",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation and Preprocessing\n",
    "\n",
    "In this section, we define the transformations that will be applied to the images before they are fed into the model:\n",
    "- **Grayscale conversion**: Convert the images to grayscale.\n",
    "- **Random augmentations**: Random resized cropping, horizontal flipping, and rotation are applied to the training images to help the model generalize better.\n",
    "- **Random Erasing**: This helps the model learn to handle occlusions and missing parts in the input images.\n",
    "- **Normalization**: Normalizing the pixel values between 0 and 1 is important for stabilizing the model's learning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "447ca4e7-1289-4bb8-b936-65c074622a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing for the training and validation sets\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.RandomErasing(p=0.2),  # Randomly erase part of the tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize for grayscale (1 channel)\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize for grayscale (1 channel)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716f193-5889-400f-bafb-91c005f158ae",
   "metadata": {},
   "source": [
    "# 3. Loading Datasets and Creating DataLoaders\n",
    "\n",
    "- **ImageFolder**: Automatically assigns labels based on the subfolder names, which represent the class names.\n",
    "- **DataLoader**: Loads batches of data from the `train` and `val` folders. It shuffles the training data to introduce randomness in batches, while validation data is loaded in a fixed order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825b3d5c-5726-44b0-b2a1-61e561dc7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"C:\\Users\\....\\Auslan_dataset\\dataset_split\\train\"\n",
    "val_dir = r\"C:\\Users\\....\\Auslan_dataset\\dataset_split\\val\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7dd5d-5f55-4374-96da-d76281057477",
   "metadata": {},
   "source": [
    "# 4. Model Setup (ResNet-18 with Grayscale Input Modification)\n",
    "\n",
    "- We load a pre-trained ResNet-18 model that has been trained on ImageNet.\n",
    "- Modify the **first convolutional layer** to accept 1-channel (grayscale) images instead of 3-channel (RGB) images.\n",
    "- Modify the **fully connected (FC) layer** to output the correct number of classes (36 in this case).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a41178-e5ec-4d91-9244-d6a539e4f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load ResNet-18 model with ImageNet pre-trained weights\n",
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify the first layer to accept grayscale input\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Modify the FC layer for 36 output classes (26 letters + 10 digits)\n",
    "num_classes = 36\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, num_classes)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc863dd-c7a0-4c10-afb5-eec9a3218940",
   "metadata": {},
   "source": [
    "# 5. Defining Loss Function and Optimizer\n",
    "\n",
    "- **CrossEntropyLoss**: This is used for multi-class classification tasks, where the model predicts one out of multiple classes.\n",
    "- **AdamW Optimizer**: Used for weight decay and stability in optimization.\n",
    "- **OneCycleLR Scheduler**: This dynamically adjusts the learning rate throughout the training process to help the model converge faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3610ba76-d5ea-424c-89a1-2588c57d3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                epochs=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bcdbd1-c19c-4f58-b484-361682432422",
   "metadata": {},
   "source": [
    "# 6. Validation Function\n",
    "\n",
    "This function performs validation on the model after each training epoch. It:\n",
    "- Disables gradient calculation using `torch.no_grad()`.\n",
    "- Calculates the validation loss and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbc4cf58-7ed6-40e2-aa58-c2a98d76ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store for classification report\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    return val_loss, val_acc, y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943365c-c653-4891-aa59-1052cf58b92a",
   "metadata": {},
   "source": [
    "# 7. Training Loop with Early Stopping and Logging confusion matrix\n",
    "\n",
    "The training loop:\n",
    "- Trains the model over a number of epochs.\n",
    "- After each epoch, it performs validation.\n",
    "- Uses early stopping if the validation accuracy does not improve for a set number of epochs (`patience`).\n",
    "- Saves the model with the best validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd478797-8f42-468b-b669-7028c7324e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=5):\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        val_loss, val_acc, y_true, y_pred = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        wandb.log({\n",
    "            \"train_loss\": running_loss / len(train_loader.dataset),\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        # Log classification metrics (F1-score, precision, recall)\n",
    "        classification_report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "        wandb.log({\n",
    "            \"precision\": classification_report_dict[\"macro avg\"][\"precision\"],\n",
    "            \"recall\": classification_report_dict[\"macro avg\"][\"recall\"],\n",
    "            \"f1-score\": classification_report_dict[\"macro avg\"][\"f1-score\"],\n",
    "            \"accuracy\": classification_report_dict[\"accuracy\"]\n",
    "        })\n",
    "\n",
    "        # Log confusion matrix (optional)\n",
    "        wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                  y_true=y_true,\n",
    "                                                                  preds=y_pred,\n",
    "                                                                  class_names=[str(i) for i in range(36)])})\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_resnet18_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {running_loss / len(train_loader.dataset):.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete. Best Val Acc: {:.2f}%\".format(best_val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc567d65-011e-4e39-9e4f-d00ad8191d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c808f3b-b5bc-4ca6-bf66-f2648215f6d8",
   "metadata": {},
   "source": [
    "# 8. Training the Model\n",
    "\n",
    "This is the final step where we train the model using the `train_model` function defined earlier. The model is saved as `resnet18_handsign_final.pth` once training is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c480444-84b8-4f5b-8cfa-7f0b7a025cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=5)\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(model.state_dict(), 'resnet18_handsign_final.pth')\n",
    "print(\"Model saved to resnet18_handsign_final.pth\")\n",
    "\n",
    "# Finalize the W&B run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
